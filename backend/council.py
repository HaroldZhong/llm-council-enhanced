"""3-stage LLM Council orchestration."""

from typing import List, Dict, Any, Tuple
from .openrouter import query_models_parallel, query_model
from .config import COUNCIL_MODELS, CHAIRMAN_MODEL
from .logger import logger
from .tools.types import EvidencePack, UsageLimits
from .tools.registry import ToolRegistry
from .tools.router import ToolRouter
from .tools.parser import ToolParser
import uuid
import json


async def stage1_collect_responses(user_query: str, models: List[str] = None, evidence_pack: EvidencePack = None) -> List[Dict[str, Any]]:
    """
    Stage 1: Collect individual responses from all council models.

    Args:
        user_query: The user's question
        models: Optional list of models to query (defaults to COUNCIL_MODELS)
        evidence_pack: Optional evidence gathered by the Steward

    Returns:
        List of dicts with 'model' and 'response' keys
    """
    target_models = models or COUNCIL_MODELS
    
    # Format evidence for the prompt if available
    evidence_context = ""
    if evidence_pack and evidence_pack.tools_used:
        # Render a compact summary with [sID] citations
        summary_lines = ["\nEVIDENCE FROM TOOL STEWARD:"]
        
        # 1. Tool Outputs
        for tool_run in evidence_pack.tools_used:
             if tool_run.status == "executed":
                 summary_lines.append(f"- {tool_run.tool_name}: {tool_run.output_summary}")
             else:
                 summary_lines.append(f"- {tool_run.tool_name} (Rejected): {tool_run.output_summary}")
        
        # 2. Key Facts with IDs
        if evidence_pack.key_facts:
            summary_lines.append("\nKEY FACTS:")
            for fact in evidence_pack.key_facts:
                summary_lines.append(f"- {fact.fact} [s{fact.source_id}] (Confidence: {fact.confidence_score})")
        
        summary_lines.append("\nINSTRUCTIONS FOR EVIDENCE:")
        summary_lines.append("1. Uses facts from the evidence above to answer the user's question.")
        summary_lines.append("2. When you use a fact, cite it using the [sID] format at the end of the sentence. Example: 'Apple stock is up [s1].'")
        summary_lines.append("3. If the evidence is insufficient, state what is unknown. Do NOT hallucinations.")
        
        evidence_context = "\n".join(summary_lines)

    prompt = f"""{user_query}

{evidence_context}"""

    messages = [{"role": "user", "content": prompt}]

    # Query all models in parallel
    responses = await query_models_parallel(target_models, messages)

    # Format results
    stage1_results = []
    for model, response in responses.items():
        if response is not None:  # Only include successful responses
            stage1_results.append({
                "model": model,
                "response": response.get('content', ''),
                "usage": response.get('usage', {})
            })

    return stage1_results


async def stage2_collect_rankings(
    user_query: str,
    stage1_results: List[Dict[str, Any]],
    models: List[str] = None
) -> Tuple[List[Dict[str, Any]], Dict[str, str]]:
    """
    Stage 2: Each model ranks the anonymized responses.

    Args:
        user_query: The original user query
        stage1_results: Results from Stage 1
        models: Optional list of models to query (defaults to COUNCIL_MODELS)

    Returns:
        Tuple of (rankings list, label_to_model mapping)
    """
    target_models = models or COUNCIL_MODELS
    
    # Create anonymized labels for responses (Response A, Response B, etc.)
    labels = [chr(65 + i) for i in range(len(stage1_results))]  # A, B, C, ...

    # Create mapping from label to model name
    label_to_model = {
        f"Response {label}": result['model']
        for label, result in zip(labels, stage1_results)
    }

    # Build the ranking prompt
    responses_text = "\n\n".join([
        f"Response {label}:\n{result['response']}"
        for label, result in zip(labels, stage1_results)
    ])

    ranking_prompt = f"""You are evaluating different responses to the following question:

Question: {user_query}

Here are the responses from different models (anonymized):

{responses_text}

Your task:
1. First, evaluate each response individually. For each response, explain what it does well and what it does poorly.
2. Then, at the very end of your response, provide a final ranking.

IMPORTANT: Your final ranking MUST be formatted EXACTLY as follows:
- Start with the line "FINAL RANKING:" (all caps, with colon)
- Then list the responses from best to worst as a numbered list
- Each line should be: number, period, space, then ONLY the response label (e.g., "1. Response A")
- Do not add any other text or explanations in the ranking section

Example of the correct format for your ENTIRE response:

Response A provides good detail on X but misses Y...
Response B is accurate but lacks depth on Z...
Response C offers the most comprehensive answer...

FINAL RANKING:
1. Response C
2. Response A
3. Response B

Now provide your evaluation and ranking:"""

    messages = [{"role": "user", "content": ranking_prompt}]

    # Get rankings from all council models in parallel
    responses = await query_models_parallel(target_models, messages)

    # Format results
    stage2_results = []
    for model, response in responses.items():
        if response is not None:
            full_text = response.get('content', '')
            parsed = parse_ranking_from_text(full_text)
            stage2_results.append({
                "model": model,
                "ranking": full_text,
                "parsed_ranking": parsed,
                "usage": response.get('usage', {})
            })

    return stage2_results, label_to_model


async def stage3_synthesize_final(
    user_query: str,
    stage1_results: List[Dict[str, Any]],
    stage2_results: List[Dict[str, Any]],
    label_to_model: Dict[str, str],
    quality_metrics: Dict[str, Dict[str, Any]],
    chairman_model: str = None
) -> Dict[str, Any]:
    """
    Stage 3: Chairman synthesizes final answer with confidence scoring.
    
    Args:
        user_query: Original user question
        stage1_results: Individual model responses
        stage2_results: Model rankings
        label_to_model: Mapping from anonymous labels to model names
        quality_metrics: Per-model quality metrics from Stage 2 rankings
        chairman_model: Optional chairman model ID
        
    Returns:
        Dict with 'response', 'model', 'usage', 'confidence', 'avg_consensus', 'quality_metrics'
    """
    target_chairman = chairman_model or CHAIRMAN_MODEL
    
    # Compute overall confidence
    confidence_label, avg_consensus = compute_overall_confidence(quality_metrics)
    consensus_details = format_consensus_details(quality_metrics)
    
    # Build comprehensive context for chairman
    # Use anonymous labels to match the rankings
    stage1_text_parts = []
    for i, result in enumerate(stage1_results):
        label = chr(65 + i)  # A, B, C...
        stage1_text_parts.append(f"Response {label}:\n{result['response']}")
    stage1_text = "\n\n".join(stage1_text_parts)

    stage2_text = "\n\n".join([
        f"Model: {result['model']}\nRanking: {result['ranking']}"
        for result in stage2_results
    ])

    chairman_prompt = f"""You are the Chairman of an LLM Council. Multiple AI models have provided responses to a user's question, and then ranked each other's responses.

Original Question: {user_query}

STAGE 1 - Individual Responses:
{stage1_text}

STAGE 2 - Peer Rankings:
{stage2_text}

Consensus summary:
Overall council confidence: {confidence_label}
{consensus_details}

Your task as Chairman is to synthesize all of this information into a single, comprehensive, accurate answer to the user's original question. Consider:
- The individual responses and their insights
- The peer rankings and what they reveal about response quality
- Any patterns of agreement or disagreement

Guidelines:
- If confidence is HIGH, you can present a unified answer.
- If confidence is MEDIUM or LOW, clearly mention that the council had mixed views and explain the main perspectives.
- Stick to what the answers actually said - do not invent new facts.

Note: The system will display "Confidence: {confidence_label}" in the UI automatically.
You may mention the confidence level in your answer, but it's not required.

Provide a clear, well-reasoned final answer that represents the council's collective wisdom:"""

    messages = [{"role": "user", "content": chairman_prompt}]

    # Query the chairman model
    logger.info(f"[STAGE3] Requesting synthesis from {target_chairman}...")
    response = await query_model(target_chairman, messages)

    if response is None:
        logger.error(f"[STAGE3] ERROR: query_model returned None")
        # Fallback if chairman fails
        return {
            "model": target_chairman,
            "response": "Error: Unable to generate final synthesis.",
            "usage": {},
            "confidence": "UNKNOWN",
            "avg_consensus": 0.0,
            "quality_metrics": quality_metrics,
        }

    logger.info(f"[STAGE3] Synthesis complete, confidence={confidence_label}")

    return {
        "model": target_chairman,
        "response": response.get('content', ''),
        "usage": response.get('usage', {}),
        "confidence": confidence_label,
        "avg_consensus": avg_consensus,
        "quality_metrics": quality_metrics,
    }


def parse_ranking_from_text(ranking_text: str) -> List[str]:
    """
    Parse the FINAL RANKING section from the model's response.

    Args:
        ranking_text: The full text response from the model

    Returns:
        List of response labels in ranked order
    """
    import re

    # Look for "FINAL RANKING:" section
    if "FINAL RANKING:" in ranking_text:
        # Extract everything after "FINAL RANKING:"
        parts = ranking_text.split("FINAL RANKING:")
        if len(parts) >= 2:
            ranking_section = parts[1]
            # Try to extract numbered list format (e.g., "1. Response A")
            # This pattern looks for: number, period, optional space, "Response X"
            numbered_matches = re.findall(r'\d+\.\s*Response [A-Z]', ranking_section)
            if numbered_matches:
                # Safely extract Response labels
                parsed = []
                for m in numbered_matches:
                    match = re.search(r'Response [A-Z]', m)
                    if match:
                        parsed.append(match.group())
                return parsed

            # Fallback: Extract all "Response X" patterns in order
            matches = re.findall(r'Response [A-Z]', ranking_section)
            return matches

    # Fallback: try to find any "Response X" patterns in order
    matches = re.findall(r'Response [A-Z]', ranking_text)
    return matches


async def extract_topics(text: str, max_topics: int = 3) -> List[str]:
    """
    Extract main topics using a fast LLM.
    Text is truncated for cost and latency.
    
    Args:
        text: Combined user question + response text
        max_topics: Maximum number of topics to extract
        
    Returns:
        List of topic strings (1-3 words each)
    """
    if not text:
        return []

    prompt = f"""Extract {max_topics} main topics or keywords from the text below.
Return only a comma separated list, no explanations.
Each topic should be 1 to 3 words.

Text:
{text[:800]}

Topics:"""

    try:
        response = await query_model(
            "google/gemini-2.5-flash",
            [{"role": "user", "content": prompt}],
            timeout=10.0,
        )
    except Exception as e:
        logger.exception("[PHASE1] extract_topics failed: %s", e)
        return []

    if response is None:
        return []

    topics_raw = (response.get("content") or "").strip()
    topics = [t.strip() for t in topics_raw.split(",") if t.strip()]

    topics = topics[:max_topics]
    logger.info("[PHASE1] Topics extracted: %s", topics)
    return topics


def calculate_quality_metrics(
    stage2_results: List[Dict[str, Any]],
    label_to_model: Dict[str, str],
) -> Dict[str, Dict[str, float]]:
    """
    Compute avg_rank and consensus_score per model from Stage 2 rankings.
    
    Args:
        stage2_results: Rankings from each model in Stage 2
        label_to_model: Mapping from anonymous labels to model names
        
    Returns:
        Dict mapping model name to quality metrics:
        - avg_rank: lower is better (1 is best)
        - consensus_score: 0-1, higher means more agreement
        - num_rankings: number of times ranked
    """
    from collections import defaultdict
    
    positions_by_model = defaultdict(list)

    for ranking in stage2_results:
        parsed = ranking.get("parsed_ranking") or []
        for position, label in enumerate(parsed, start=1):
            model = label_to_model.get(label)
            if model:
                positions_by_model[model].append(position)

    quality_metrics: Dict[str, Dict[str, float]] = {}

    for model, positions in positions_by_model.items():
        if not positions:
            continue

        avg_rank = sum(positions) / len(positions)

        variance = sum((p - avg_rank) ** 2 for p in positions) / len(positions)
        consensus_score = 1.0 / (1.0 + variance)

        quality_metrics[model] = {
            "avg_rank": round(avg_rank, 2),
            "consensus_score": round(consensus_score, 2),
            "num_rankings": len(positions),
        }

    logger.info("[PHASE1] Quality metrics: %s", quality_metrics)
    return quality_metrics


def compute_overall_confidence(
    quality_metrics: Dict[str, Dict[str, Any]]
) -> tuple[str, float]:
    """
    Compute overall consensus based on per model consensus_score.

    Returns (label, avg_consensus) where:
      - label in {"HIGH", "MEDIUM", "LOW", "UNKNOWN"}
      - avg_consensus is 0 to 1
    """
    if not quality_metrics:
        logger.info("[PHASE1] No quality_metrics - confidence=UNKNOWN")
        return "UNKNOWN", 0.0

    scores = [m.get("consensus_score", 0.0) for m in quality_metrics.values()]
    if not scores:
        logger.info("[PHASE1] Empty scores in quality_metrics - confidence=UNKNOWN")
        return "UNKNOWN", 0.0

    avg_consensus = sum(scores) / len(scores)

    if avg_consensus > 0.75:
        label = "HIGH"
    elif avg_consensus > 0.5:
        label = "MEDIUM"
    else:
        label = "LOW"

    logger.info(
        "[PHASE1] Confidence computed: label=%s avg_consensus=%.3f",
        label,
        avg_consensus,
    )
    return label, avg_consensus


def format_consensus_details(
    quality_metrics: Dict[str, Dict[str, Any]]
) -> str:
    """
    Format quality metrics into human-readable consensus details.
    """
    if not quality_metrics:
        return ""

    lines = []
    for model, metrics in sorted(
        quality_metrics.items(),
        key=lambda item: item[1].get("avg_rank", 999),
    ):
        lines.append(
            f"- {model}: avg rank {metrics.get('avg_rank')}, "
            f"consensus {metrics.get('consensus_score')}"
        )

    return "\n".join(lines)


def calculate_aggregate_rankings(
    stage2_results: List[Dict[str, Any]],
    label_to_model: Dict[str, str]
) -> List[Dict[str, Any]]:
    """
    Calculate aggregate rankings across all models.

    Args:
        stage2_results: Rankings from each model
        label_to_model: Mapping from anonymous labels to model names

    Returns:
        List of dicts with model name and average rank, sorted best to worst
    """
    from collections import defaultdict

    # Track positions for each model
    model_positions = defaultdict(list)

    for ranking in stage2_results:
        ranking_text = ranking['ranking']

        # Parse the ranking from the structured format
        parsed_ranking = parse_ranking_from_text(ranking_text)

        for position, label in enumerate(parsed_ranking, start=1):
            if label in label_to_model:
                model_name = label_to_model[label]
                model_positions[model_name].append(position)

    # Calculate average position for each model
    aggregate = []
    for model, positions in model_positions.items():
        if positions:
            avg_rank = sum(positions) / len(positions)
            aggregate.append({
                "model": model,
                "average_rank": round(avg_rank, 2),
                "rankings_count": len(positions)
            })

    # Sort by average rank (lower is better)
    aggregate.sort(key=lambda x: x['average_rank'])

    return aggregate


async def generate_conversation_title(user_query: str) -> str:
    """
    Generate a short title for a conversation based on the first user message.

    Args:
        user_query: The first user message

    Returns:
        A short title (3-5 words)
    """
    title_prompt = f"""Generate a very short title (3-5 words maximum) that summarizes the following question.
The title should be concise and descriptive. Do not use quotes or punctuation in the title.

Question: {user_query}

Title:"""

    messages = [{"role": "user", "content": title_prompt}]

    # Use gemini-2.5-flash for title generation (fast and cheap)
    response = await query_model("google/gemini-2.5-flash", messages, timeout=30.0)

    if response is None:
        # Fallback to a generic title
        return "New Conversation"

    title = response.get('content', 'New Conversation').strip()

    # Clean up the title - remove quotes, limit length
    title = title.strip('"\'')

    # Truncate if too long
    if len(title) > 50:
        title = title[:47] + "..."

    return title


async def run_tool_steward_phase(user_query: str, run_id: str, chairman_model: str = None) -> Tuple[EvidencePack, Dict[str, Any]]:
    """
    Stage 0: Tool Steward decides and executes tools.
    Returns: (EvidencePack, usage_dict)
    """
    target_model = chairman_model or CHAIRMAN_MODEL
    logger.info(f"[STEWARD] Starting phase for run {run_id}")

    # 1. Dynamic Prompting
    tool_descriptions = ToolRegistry.to_prompt_format()
    
    steward_prompt = f"""You are the Tool Steward for an AI Council.
Your job is to decide if tools are needed to answer the user's question, and if so, which ones.

User Question: {user_query}

{tool_descriptions}

INSTRUCTIONS:
1. Analyze the detailed question.
2. Select the most relevant tools from the list above.
3. Return a JSON object with your decision.

FORMAT (JSON ONLY):
{{
  "action": "use_tools" | "no_tools",
  "reason": "Why you made this decision",
  "calls": [
    {{
      "name": "tool.name",
      "arguments": {{ "arg": "value" }},
      "purpose": "Why this specific call is needed",
      "priority": "high" | "normal" | "low"
    }}
  ]
}}

If no tools are needed (e.g., for general chit-chat or pure logic questions), return action="no_tools".
"""

    messages = [{"role": "user", "content": steward_prompt}]
    
    # 2. Query Model
    response = await query_model(target_model, messages, timeout=60.0)
    usage = response.get("usage", {}) if response else {}
    
    # 3. Parse & Router
    parser = ToolParser()
    parsed_data = {"action": "no_tools"} # Fallback
    
    if response and response.get("content"):
        parsed_data = parser.parse_steward_output(response["content"])
    
    # 4. Execute Logic
    router = ToolRouter(
        allowlist=["web.search", "web.fetch", "finance.quote"], # Explicit allowlist
        max_calls_per_run=3
    )

    if parsed_data.get("action") == "use_tools" and parsed_data.get("calls"):
        logger.info(f"[STEWARD] Decided to use tools: {len(parsed_data['calls'])} calls")
        from .tools.types import ToolCall
        
        tool_calls = []
        for call_dict in parsed_data["calls"]:
            try:
                tool_calls.append(ToolCall(
                    run_id=run_id,
                    name=call_dict.get("name"),
                    arguments=call_dict.get("arguments", {}),
                    purpose=call_dict.get("purpose", "Unknown"),
                    priority=call_dict.get("priority", "normal"),
                    requested_by=target_model
                ))
            except Exception as e:
                logger.warning(f"[STEWARD] internal error parsing tool call: {e}")
        
        evidence_pack = await router.execute_tool_calls(tool_calls, run_id)
        
    else:
        logger.info("[STEWARD] No tools needed.")
        # Return empty pack
        evidence_pack = EvidencePack(
            run_id=run_id,
            query=user_query,
            tools_used=[],
            key_facts=[],
            limits=UsageLimits()
        )
        
    return evidence_pack, usage


async def run_full_council(
    user_query: str, 
    council_models: List[str] = None, 
    chairman_model: str = None
) -> Tuple[List, List, Dict, Dict, EvidencePack]:
    """
    Run the complete 3-stage council process (now with Stage 0 Steward).

    Args:
        user_query: The user's question
        council_models: Optional list of council models (defaults to COUNCIL_MODELS)
        chairman_model: Optional chairman model (defaults to CHAIRMAN_MODEL)

    Returns:
        Tuple of (stage1_results, stage2_results, stage3_result, metadata, evidence_pack)
    """
    run_id = str(uuid.uuid4())
    logger.info(f"[COUNCIL] Starting run {run_id}")

    # Stage 0: Tool Steward
    evidence_pack, steward_usage = await run_tool_steward_phase(user_query, run_id, chairman_model)

    # Stage 1: Collect individual responses (with evidence)
    stage1_results = await stage1_collect_responses(user_query, council_models, evidence_pack)

    # If no models responded successfully, return error
    if not stage1_results:
        return [], [], {
            "model": "error",
            "response": "All models failed to respond. Please try again."
        }, {}

    # Stage 2: Collect rankings
    stage2_results, label_to_model = await stage2_collect_rankings(user_query, stage1_results, council_models)

    # Calculate aggregate rankings
    aggregate_rankings = calculate_aggregate_rankings(stage2_results, label_to_model)
    
    # Calculate quality metrics for confidence scoring
    quality_metrics = calculate_quality_metrics(stage2_results, label_to_model)

    # Stage 3: Synthesize final answer with confidence
    stage3_result = await stage3_synthesize_final(
        user_query,
        stage1_results,
        stage2_results,
        label_to_model,
        quality_metrics,
        chairman_model
    )

    # Prepare metadata
    metadata = {
        "label_to_model": label_to_model,
        "aggregate_rankings": aggregate_rankings
    }

    return stage1_results, stage2_results, stage3_result, metadata, evidence_pack


async def rewrite_query(query: str, conversation_history: List[Dict[str, str]]) -> str:
    """
    Rewrite a query to be self-contained by resolving coreferences.
    Uses conversation history to expand pronouns and references.
    
    Args:
        query: The user's query (may contain pronouns like "it", "its", "that")
        conversation_history: List of previous messages
        
    Returns:
        Rewritten self-contained query, or original if rewriting fails
    """
    from .config import ENABLE_QUERY_REWRITE
    
    if not ENABLE_QUERY_REWRITE:
        return query
    
    # Heuristic: skip if query looks self-contained (>10 words)
    word_count = len(query.split())
    if word_count > 10:
        logger.info("[PHASE1] Query looks self-contained, skipping rewrite")
        return query
    
    # Skip if no context available
    if not conversation_history or len(conversation_history) < 2:
        logger.info("[PHASE1] No context available, skipping rewrite")
        return query
    
    # Build context from last 2 turns
    context_parts = []
    for msg in conversation_history[-4:]:  # Last 2 turns (user + assistant)
        role = msg.get("role", "")
        content = msg.get("content", "")
        
        # For assistant messages, use stage3 response if available
        if role == "assistant":
            stage3 = msg.get("stage3", {})
            content = stage3.get("response", content)
        
        if content:
            context_parts.append(f"{role}: {content[:200]}")  # Truncate for cost
    
    context = "\n".join(context_parts)
    
    prompt = f"""Rewrite the user's question to be self-contained by replacing pronouns (it, its, that, they, etc.) with the specific topics from the conversation.

Conversation context:
{context}

User's question: {query}

Rewritten question (ONE sentence, no explanations):"""
    
    try:
        response = await query_model(
            "google/gemini-2.5-flash",
            [{"role": "user", "content": prompt}],
            timeout=10.0,
        )
        
        if response and response.get("content"):
            rewritten = response["content"].strip()
            
            # Extract just the first meaningful line if multiple options given
            if "\n" in rewritten or "option" in rewritten.lower():
                lines = [line.strip() for line in rewritten.split("\n") if line.strip()]
                # Find first line that looks like a question
                for line in lines:
                    # Skip meta-commentary
                    if any(skip in line.lower() for skip in ["option", "rewriting", "here are", "could be"]):
                        continue
                    if len(line) > 10:  # Reasonable length
                        rewritten = line
                        break
                else:
                    rewritten = lines[0] if lines else rewritten
            
            # Remove common prefixes
            for prefix in ["> ", "* ", "- ", "â€¢ ", "**"]:
                if rewritten.startswith(prefix):
                    rewritten = rewritten[len(prefix):].lstrip()
            
            logger.info("[PHASE1] Query rewrite: original=%r", query)
            logger.info("[PHASE1] Query rewrite: rewritten=%r", rewritten)
            return rewritten
        else:
            logger.info("[PHASE1] Query rewrite failed, using original")
            return query
            
    except Exception as e:
        logger.error("[PHASE1] Query rewrite error: %s", e)
        return query


async def chat_with_chairman(
    user_query: str,
    conversation_history: List[Dict[str, Any]],
    rag_context: str = "",
    chairman_model: str = None
) -> Dict[str, Any]:
    """
    Chat directly with the Chairman, using RAG-retrieved context.

    Args:
        user_query: The user's new question
        conversation_history: Full history of the conversation
        rag_context: Relevant context retrieved from ChromaDB
        chairman_model: Optional chairman model ID (defaults to CHAIRMAN_MODEL)

    Returns:
        Dict with 'content' and optional 'reasoning' (chain of thought)
    """
    target_chairman = chairman_model or CHAIRMAN_MODEL
    messages = []
    
    # System prompt to set the persona and inject RAG context
    system_prompt = """You are the Chairman of the AI Council. 
You have previously presided over a council of AI models who debated and ranked answers to the user's questions.
Your goal now is to answer follow-up questions from the user.

You may optionally receive previous council deliberations for this conversation.
Use them only if they are relevant to the user's question.
Do not repeat old answers verbatim; instead, build on them.

"""

    if rag_context:
        system_prompt += f"""Relevant previous council outputs (may be partial):
{rag_context}

Guidance on context labels:
- If a chunk is labeled 'synthesis', treat it as a previous final decision.
- If a chunk is labeled 'opinion', treat it as a single model's draft answer, not consensus.
- If a chunk is labeled 'review', treat it as an evaluation of other models' answers.
"""

    system_prompt += "\nBe helpful, authoritative, and transparent about the council's reasoning."

    messages.append({"role": "system", "content": system_prompt})

    # Build context from history (simplified for chat)
    for msg in conversation_history:
        role = msg.get("role")
        
        if role == "user":
            messages.append({"role": "user", "content": msg.get("content", "")})
            
        elif role == "assistant":
            # Check if this was a full council turn
            if "stage3" in msg:
                # Only include the final response in the immediate history
                # Details are in RAG if needed
                final_response = msg["stage3"].get("response", "")
                messages.append({"role": "assistant", "content": final_response})
                
            elif "content" in msg:
                # Standard chat message
                messages.append({"role": "assistant", "content": msg["content"]})

    # Add the new user query
    messages.append({"role": "user", "content": user_query})

    # Query the chairman
    logger.info(f"[CHAIRMAN] Calling {target_chairman} with {len(messages)} messages...")
    import time
    start_time = time.time()
    response = await query_model(target_chairman, messages)
    elapsed = time.time() - start_time
    logger.info(f"[CHAIRMAN] Response received in {elapsed:.2f}s")
    
    if response is None:
        logger.error(f"[CHAIRMAN] ERROR: query_model returned None")
        return {
            "content": "I apologize, but I am unable to respond at this moment."
        }
        
    content = response.get("content", "")
    reasoning = response.get("reasoning_details")
    usage = response.get("usage", {})
    
    result = {"content": content, "usage": usage}
    
    if reasoning:
        # Handle both string (from new extract_reasoning) and list (legacy API format)
        if isinstance(reasoning, str):
            # New format: reasoning is already extracted as a string
            logger.info(f"[CHAIRMAN] Model provided reasoning ({len(reasoning)} chars)")
            result["reasoning"] = reasoning
        elif isinstance(reasoning, list):
            # Legacy format: reasoning_details is a list of steps
            logger.info(f"[CHAIRMAN] Model provided reasoning with {len(reasoning)} steps")
            # Extract text reasoning if available
            reasoning_text = None
            for step in reasoning:
                if step.get("type") == "reasoning.text":
                    reasoning_text = step.get("text", "")
                    break
            
            if reasoning_text:
                result["reasoning"] = reasoning_text
    
    return result

